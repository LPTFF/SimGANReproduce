{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7XF_BwwRIYF",
    "colab_type": "text"
   },
   "source": [
    "挂载kaggle数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mwNwLvFxMx7f",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rRn619gjQ2Y3",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"/root/.kaggle\") \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "jhZmslBhRuC7",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "uJP_FXzSQ7z9",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()#上传kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "MbBIiubuM1dO",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104.0
    },
    "outputId": "0c18d178-44df-4990-81f7-c67f731285c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading eye-gaze.zip to /content\n",
      "100% 4.31G/4.31G [00:35<00:00, 134MB/s]\n",
      "100% 4.31G/4.31G [00:35<00:00, 132MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d 4quant/eye-gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CWmiASy0OYWD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !mkdir /root/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "pzUKVRYARgeW",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151.0
    },
    "outputId": "fae2a428-45b4-43bd-f938-9225ce8a04d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  eye-gaze.zip\n",
      "  inflating: normalized.zip          \n",
      "  inflating: real_gaze.h5            \n",
      "  inflating: original.zip            \n",
      "  inflating: gaze.h5                 \n",
      "  inflating: gaze.csv                \n",
      "  inflating: gaze.json               \n",
      "  inflating: MPIIGaze.zip            \n"
     ]
    }
   ],
   "source": [
    "!unzip eye-gaze.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykoAdR4foahQ",
    "colab_type": "text"
   },
   "source": [
    "开始调试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rYP2nyvooHtr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"content\") \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "6juz5gyp_-II",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# !rm -rf SimGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GSASLzy4oRe4",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "mRq46NMSV2hA",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "bdf409cb-cf32-4054-e296-4c8a14d52dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-version 1.12.0-rc1 keras-version 2.1.6\n"
     ]
    }
   ],
   "source": [
    "print('tf-version',tf.__version__, 'keras-version', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "gaONosEcV5L-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118.0
    },
    "outputId": "f45a9e9e-4ee8-4c52-a98c-d931c6885afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'look_vec', 'path']\n",
      "Synthetic images found: 50000\n",
      "image ..\\..\\..\\..\\Downloads\\UnityEyes_Windows\\UnityEyes_Windows\\imgs\\1.jpg shape: (35, 55)\n",
      "['image']\n",
      "Real Images found: 34100\n",
      "image ..\\data\\MPIIGaze_Dataset\\000055ed-890a-411f-be6e-b8453b495389.png shape: (35, 55)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.dirname(os.path.abspath('.'))\n",
    "data_dir = os.path.join('..', '/content')\n",
    "cache_dir = '.'\n",
    "\n",
    "# load the data file and extract dimensions\n",
    "with h5py.File(os.path.join(data_dir,'gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    assert 'look_vec' in t_file, \"Look vector is missing\"\n",
    "    assert 'path' in t_file, \"Paths are missing\"\n",
    "    print('Synthetic images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    syn_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "with h5py.File(os.path.join(data_dir,'real_gaze.h5'),'r') as t_file:\n",
    "    print(list(t_file.keys()))\n",
    "    assert 'image' in t_file, \"Images are missing\"\n",
    "    print('Real Images found:',len(t_file['image']))\n",
    "    for _, (ikey, ival) in zip(range(1), t_file['image'].items()):\n",
    "        print('image',ikey,'shape:',ival.shape)\n",
    "        img_height, img_width = ival.shape\n",
    "        img_channels = 1\n",
    "    real_image_stack = np.stack([np.expand_dims(a,-1) for a in t_file['image'].values()],0)\n",
    "\n",
    "#\n",
    "# training params\n",
    "#\n",
    "\n",
    "nb_steps = 20 # originally 10000, but this makes the kernel time out\n",
    "batch_size = 49\n",
    "k_d = 1  # number of discriminator updates per step\n",
    "k_g = 2  # number of generative network updates per step\n",
    "log_interval = 100\n",
    "pre_steps = 15 # for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BW0BOjTeWYgM",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Module to plot a batch of images along w/ their corresponding label(s)/annotations and save the plot to disc.\n",
    "\n",
    "Use cases:\n",
    "Plot images along w/ their corresponding ground-truth label & model predicted label,\n",
    "Plot images generated by a GAN along w/ any annotations used to generate these images,\n",
    "Plot synthetic, generated, refined, and real images and see how they compare as training progresses in a GAN,\n",
    "etc...\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from itertools import groupby\n",
    "from skimage.util.montage import montage2d\n",
    "def plot_batch(image_batch, figure_path, label_batch=None):\n",
    "    \n",
    "    all_groups = {label: montage2d(np.stack([img[:,:,0] for img, lab in img_lab_list],0)) \n",
    "                  for label, img_lab_list in groupby(zip(image_batch, label_batch), lambda x: x[1])}\n",
    "    fig, c_axs = plt.subplots(1,len(all_groups), figsize=(len(all_groups)*4, 8), dpi = 600)\n",
    "    for c_ax, (c_label, c_mtg) in zip(c_axs, all_groups.items()):\n",
    "        c_ax.imshow(c_mtg, cmap='bone')\n",
    "        c_ax.set_title(c_label)\n",
    "        c_ax.axis('off')\n",
    "    fig.savefig(os.path.join(figure_path))\n",
    "    plt.close()\n",
    "\"\"\"\n",
    "Module implementing the image history buffer described in `2.3. Updating Discriminator using a History of\n",
    "Refined Images` of https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "\"\"\"\n",
    "class ImageHistoryBuffer(object):\n",
    "    def __init__(self, shape, max_size, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the class's state.\n",
    "\n",
    "        :param shape: Shape of the data to be stored in the image history buffer\n",
    "                      (i.e. (0, img_height, img_width, img_channels)).\n",
    "        :param max_size: Maximum number of images that can be stored in the image history buffer.\n",
    "        :param batch_size: Batch size used to train GAN.\n",
    "        \"\"\"\n",
    "        self.image_history_buffer = np.zeros(shape=shape)\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_to_image_history_buffer(self, images, nb_to_add=None):\n",
    "        \"\"\"\n",
    "        To be called during training of GAN. By default add batch_size // 2 images to the image history buffer each\n",
    "        time the generator generates a new batch of images.\n",
    "\n",
    "        :param images: Array of images (usually a batch) to be added to the image history buffer.\n",
    "        :param nb_to_add: The number of images from `images` to add to the image history buffer\n",
    "                          (batch_size / 2 by default).\n",
    "        \"\"\"\n",
    "        if not nb_to_add:\n",
    "            nb_to_add = self.batch_size // 2\n",
    "\n",
    "        if len(self.image_history_buffer) < self.max_size:\n",
    "            np.append(self.image_history_buffer, images[:nb_to_add], axis=0)\n",
    "        elif len(self.image_history_buffer) == self.max_size:\n",
    "            self.image_history_buffer[:nb_to_add] = images[:nb_to_add]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        np.random.shuffle(self.image_history_buffer)\n",
    "\n",
    "    def get_from_image_history_buffer(self, nb_to_get=None):\n",
    "        \"\"\"\n",
    "        Get a random sample of images from the history buffer.\n",
    "\n",
    "        :param nb_to_get: Number of images to get from the image history buffer (batch_size / 2 by default).\n",
    "        :return: A random sample of `nb_to_get` images from the image history buffer, or an empty np array if the image\n",
    "                 history buffer is empty.\n",
    "        \"\"\"\n",
    "        if not nb_to_get:\n",
    "            nb_to_get = self.batch_size // 2\n",
    "\n",
    "        try:\n",
    "            return self.image_history_buffer[:nb_to_get]\n",
    "        except IndexError:\n",
    "            return np.zeros(shape=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JIidTk5dWcob",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def refiner_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The refiner network, Rθ, is a residual network (ResNet). It modifies the synthetic image on a pixel level, rather\n",
    "    than holistically modifying the image content, preserving the global structure and annotations.\n",
    "\n",
    "    :param input_image_tensor: Input tensor that corresponds to a synthetic image.\n",
    "    :return: Output tensor that corresponds to a refined synthetic image.\n",
    "    \"\"\"\n",
    "    def resnet_block(input_features, nb_features=64, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "        \"\"\"\n",
    "        A ResNet block with two `nb_kernel_rows` x `nb_kernel_cols` convolutional layers,\n",
    "        each with `nb_features` feature maps.\n",
    "\n",
    "        See Figure 6 in https://arxiv.org/pdf/1612.07828v1.pdf.\n",
    "\n",
    "        :param input_features: Input tensor to ResNet block.\n",
    "        :return: Output tensor from ResNet block.\n",
    "        \"\"\"\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(input_features)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Convolution2D(nb_features, nb_kernel_rows, nb_kernel_cols, border_mode='same')(y)\n",
    "\n",
    "        y = layers.merge([input_features, y], mode='sum')\n",
    "        return layers.Activation('relu')(y)\n",
    "\n",
    "    # an input image of size w × h is convolved with 3 × 3 filters that output 64 feature maps\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', activation='relu')(input_image_tensor)\n",
    "\n",
    "    # the output is passed through 4 ResNet blocks\n",
    "    for _ in range(4):\n",
    "        x = resnet_block(x)\n",
    "\n",
    "    # the output of the last ResNet block is passed to a 1 × 1 convolutional layer producing 1 feature map\n",
    "    # corresponding to the refined synthetic image\n",
    "    return layers.Convolution2D(img_channels, 1, 1, border_mode='same', activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vl_0c-SrWgHr",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def discriminator_network(input_image_tensor):\n",
    "    \"\"\"\n",
    "    The discriminator network, Dφ, contains 5 convolution layers and 2 max-pooling layers.\n",
    "\n",
    "    :param input_image_tensor: Input tensor corresponding to an image, either real or refined.\n",
    "    :return: Output tensor that corresponds to the probability of whether an image is real or refined.\n",
    "    \"\"\"\n",
    "    x = layers.Convolution2D(96, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(input_image_tensor)\n",
    "    x = layers.Convolution2D(64, 3, 3, border_mode='same', subsample=(2, 2), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), border_mode='same', strides=(1, 1))(x)\n",
    "    x = layers.Convolution2D(32, 3, 3, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(32, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "    x = layers.Convolution2D(2, 1, 1, border_mode='same', subsample=(1, 1), activation='relu')(x)\n",
    "\n",
    "    # here one feature map corresponds to `is_real` and the other to `is_refined`,\n",
    "    # and the custom loss function is then `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "    return layers.Reshape((-1, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "h7T28wn7WjW1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2053.0
    },
    "outputId": "6d4e65bc-7bce-4f7f-8da6-195637ebf7d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.6/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (1, 1), activation=\"tanh\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(2, 2), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 35, 55, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 35, 55, 64)   640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 35, 55, 64)   36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 35, 55, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 35, 55, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 35, 55, 64)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 35, 55, 64)   0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 35, 55, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 35, 55, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 35, 55, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 35, 55, 64)   0           activation_2[0][0]               \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 35, 55, 64)   0           merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 55, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 35, 55, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 55, 64)   36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 35, 55, 64)   0           activation_4[0][0]               \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 55, 64)   0           merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 55, 64)   36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 55, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 55, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_4 (Merge)                 (None, 35, 55, 64)   0           activation_6[0][0]               \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 55, 64)   0           merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 55, 1)    65          activation_8[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 296,129\n",
      "Trainable params: 296,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 18, 28, 96)        960       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 9, 14, 64)         55360     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 14, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 9, 14, 32)         18464     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 14, 32)         1056      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 14, 2)          66        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 126, 2)            0         \n",
      "=================================================================\n",
      "Total params: 75,906\n",
      "Trainable params: 75,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 35, 55, 1)         0         \n",
      "_________________________________________________________________\n",
      "refiner (Model)              (None, 35, 55, 1)         296129    \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 126, 2)            75906     \n",
      "=================================================================\n",
      "Total params: 372,035\n",
      "Trainable params: 372,035\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 1), activation=\"relu\", strides=(1, 1), padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"refiner\", inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"discriminator\", inputs=Tensor(\"in..., outputs=Tensor(\"re...)`\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"combined\", inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# define model input and output tensors\n",
    "#\n",
    "\n",
    "synthetic_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "refined_image_tensor = refiner_network(synthetic_image_tensor)\n",
    "\n",
    "refined_or_real_image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "discriminator_output = discriminator_network(refined_or_real_image_tensor)\n",
    "\n",
    "#\n",
    "# define models\n",
    "#\n",
    "\n",
    "refiner_model = models.Model(input=synthetic_image_tensor, output=refined_image_tensor, name='refiner')\n",
    "discriminator_model = models.Model(input=refined_or_real_image_tensor, output=discriminator_output,\n",
    "                                   name='discriminator')\n",
    "\n",
    "# combined must output the refined image along w/ the disc's classification of it for the refiner's self-reg loss\n",
    "refiner_model_output = refiner_model(synthetic_image_tensor)\n",
    "combined_output = discriminator_model(refiner_model_output)\n",
    "combined_model = models.Model(input=synthetic_image_tensor, output=[refiner_model_output, combined_output],\n",
    "                              name='combined')\n",
    "\n",
    "discriminator_model_output_shape = discriminator_model.output_shape\n",
    "\n",
    "print(refiner_model.summary())\n",
    "print(discriminator_model.summary())\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "Ywz0raNaWmn8",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "b9cfe6a2-71f9-451d-e6e0-4f6c91c4e5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running the patched version of keras/pydot!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "# Define model\n",
    "try:\n",
    "    model_to_dot(refiner_model, show_shapes=True).write_svg('refiner_model.svg')\n",
    "    SVG('refiner_model.svg')\n",
    "except ImportError:\n",
    "    print('Not running the patched version of keras/pydot!')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "M-hn4St_W26D",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model_to_dot(discriminator_model, show_shapes=True).write_svg('discriminator_model.svg')\n",
    "    SVG('discriminator_model.svg')\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "UsC2uj8zW4kX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#\n",
    "# define custom l1 loss function for the refiner\n",
    "#\n",
    "\n",
    "def self_regularization_loss(y_true, y_pred):\n",
    "    delta = 0.0001  # FIXME: need to figure out an appropriate value for this\n",
    "    return tf.multiply(delta, tf.reduce_sum(tf.abs(y_pred - y_true)))\n",
    "\n",
    "#\n",
    "# define custom local adversarial loss (softmax for each image section) for the discriminator\n",
    "# the adversarial loss function is the sum of the cross-entropy losses over the local patches\n",
    "#\n",
    "\n",
    "def local_adversarial_loss(y_true, y_pred):\n",
    "    # y_true and y_pred have shape (batch_size, # of local patches, 2), but really we just want to average over\n",
    "    # the local patches and batch size so we can reshape to (batch_size * # of local patches, 2)\n",
    "    y_true = tf.reshape(y_true, (-1, 2))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 2))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "rev_w5X2W8Fq",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171.0
    },
    "outputId": "1d99d21d-c66b-4b75-bd55-8a7e36a0c96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-80-4b79cff1ed6f>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=1e-3)\n",
    "\n",
    "refiner_model.compile(optimizer=sgd, loss=self_regularization_loss)\n",
    "discriminator_model.compile(optimizer=sgd, loss=local_adversarial_loss)\n",
    "discriminator_model.trainable = False\n",
    "combined_model.compile(optimizer=sgd, loss=[self_regularization_loss, local_adversarial_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4I-_Xjf1XBuU",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "refiner_model_path = None\n",
    "discriminator_model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "CvhlJCJOXE1E",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator(\n",
    "    preprocessing_function=applications.xception.preprocess_input,\n",
    "    data_format='channels_last')\n",
    "\n",
    "flow_from_directory_params = {'target_size': (img_height, img_width),\n",
    "                              'color_mode': 'grayscale' if img_channels == 1 else 'rgb',\n",
    "                              'class_mode': None,\n",
    "                              'batch_size': batch_size}\n",
    "flow_params = {'batch_size': batch_size}\n",
    "\n",
    "synthetic_generator = datagen.flow(\n",
    "    x = syn_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "real_generator = datagen.flow(\n",
    "    x = real_image_stack,\n",
    "    **flow_params\n",
    ")\n",
    "\n",
    "def get_image_batch(generator):\n",
    "    \"\"\"keras generators may generate an incomplete batch for the last batch\"\"\"\n",
    "    img_batch = generator.next()\n",
    "    if len(img_batch) != batch_size:\n",
    "        img_batch = generator.next()\n",
    "\n",
    "    assert len(img_batch) == batch_size\n",
    "\n",
    "    return img_batch\n",
    "\n",
    "# the target labels for the cross-entropy loss layer are 0 for every yj (real) and 1 for every xi (refined)\n",
    "y_real = np.array([[[1.0, 0.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "y_refined = np.array([[[0.0, 1.0]] * discriminator_model_output_shape[1]] * batch_size)\n",
    "assert y_real.shape == (batch_size, discriminator_model_output_shape[1], 2)\n",
    "batch_out = get_image_batch(synthetic_generator)\n",
    "assert batch_out.shape == (batch_size, img_height, img_width, img_channels), \"Image Dimensions do not match, {}!={}\".format(batch_out.shape, (batch_size, img_height, img_width, img_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "l6k_Usd5XKLL",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154.0
    },
    "outputId": "a06fdd91-f4a7-497f-eb16-9384ac06de1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-training the refiner network...\n",
      "Saving batch of refined images during pre-training at step: 0.\n",
      "Refiner model self regularization loss: [0.03115231].\n",
      "pre-training the discriminator network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator model loss: [0.10441813].\n"
     ]
    }
   ],
   "source": [
    "if not refiner_model_path:\n",
    "    # we first train the Rθ network with just self-regularization loss for 1,000 steps\n",
    "    print('pre-training the refiner network...')\n",
    "    gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    for i in range(pre_steps):\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        gen_loss = np.add(refiner_model.train_on_batch(synthetic_image_batch, synthetic_image_batch), gen_loss)\n",
    "\n",
    "        # log every `log_interval` steps\n",
    "        if not i % log_interval:\n",
    "            figure_name = 'refined_image_batch_pre_train_step_{}.png'.format(i)\n",
    "            print('Saving batch of refined images during pre-training at step: {}.'.format(i))\n",
    "\n",
    "            synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "            plot_batch(\n",
    "                np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "                os.path.join(cache_dir, figure_name),\n",
    "                label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "            print('Refiner model self regularization loss: {}.'.format(gen_loss / log_interval))\n",
    "            gen_loss = np.zeros(shape=len(refiner_model.metrics_names))\n",
    "\n",
    "    refiner_model.save(os.path.join(cache_dir, 'refiner_model_pre_trained.h5'))\n",
    "else:\n",
    "    refiner_model.load_weights(refiner_model_path)\n",
    "\n",
    "if not discriminator_model_path:\n",
    "    # and Dφ for 200 steps (one mini-batch for refined images, another for real)\n",
    "    print('pre-training the discriminator network...')\n",
    "    disc_loss = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "    for _ in range(pre_steps):\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss)\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "        disc_loss = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined), disc_loss)\n",
    "\n",
    "    discriminator_model.save(os.path.join(cache_dir, 'discriminator_model_pre_trained.h5'))\n",
    "\n",
    "    # hard-coded for now\n",
    "    print('Discriminator model loss: {}.'.format(disc_loss / (100 * 2)))\n",
    "else:\n",
    "    discriminator_model.load_weights(discriminator_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "fI209jUxXQSz",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474.0
    },
    "outputId": "ece62371-a574-4fbb-b757-2a8c5574b149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 of 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving batch of refined images at adversarial step: 0.\n",
      "Refiner model loss: [0.01520807 0.00821895 0.00698912].\n",
      "Discriminator model loss real: [0.0035204].\n",
      "Discriminator model loss refined: [0.00343915].\n",
      "Step: 1 of 20.\n",
      "Step: 2 of 20.\n",
      "Step: 3 of 20.\n",
      "Step: 4 of 20.\n",
      "Step: 5 of 20.\n",
      "Step: 6 of 20.\n",
      "Step: 7 of 20.\n",
      "Step: 8 of 20.\n",
      "Step: 9 of 20.\n",
      "Step: 10 of 20.\n",
      "Step: 11 of 20.\n",
      "Step: 12 of 20.\n",
      "Step: 13 of 20.\n",
      "Step: 14 of 20.\n",
      "Step: 15 of 20.\n",
      "Step: 16 of 20.\n",
      "Step: 17 of 20.\n",
      "Step: 18 of 20.\n",
      "Step: 19 of 20.\n"
     ]
    }
   ],
   "source": [
    "# TODO: what is an appropriate size for the image history buffer?\n",
    "image_history_buffer = ImageHistoryBuffer((0, img_height, img_width, img_channels), batch_size * 100, batch_size)\n",
    "\n",
    "combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "# see Algorithm 1 in https://arxiv.org/pdf/1612.07828v1.pdf\n",
    "for i in range(nb_steps):\n",
    "    print('Step: {} of {}.'.format(i, nb_steps))\n",
    "\n",
    "    # train the refiner\n",
    "    for _ in range(k_g * 2):\n",
    "        # sample a mini-batch of synthetic images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "\n",
    "        # update θ by taking an SGD step on mini-batch loss LR(θ)\n",
    "        combined_loss = np.add(combined_model.train_on_batch(synthetic_image_batch,\n",
    "                                                             [synthetic_image_batch, y_real]), combined_loss)\n",
    "\n",
    "    for _ in range(k_d):\n",
    "        # sample a mini-batch of synthetic and real images\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        real_image_batch = get_image_batch(real_generator)\n",
    "\n",
    "        # refine the synthetic images w/ the current refiner\n",
    "        refined_image_batch = refiner_model.predict_on_batch(synthetic_image_batch)\n",
    "\n",
    "        # use a history of refined images\n",
    "        half_batch_from_image_history = image_history_buffer.get_from_image_history_buffer()\n",
    "        image_history_buffer.add_to_image_history_buffer(refined_image_batch)\n",
    "\n",
    "        if len(half_batch_from_image_history):\n",
    "            refined_image_batch[:batch_size // 2] = half_batch_from_image_history\n",
    "\n",
    "        # update φ by taking an SGD step on mini-batch loss LD(φ)\n",
    "        disc_loss_real = np.add(discriminator_model.train_on_batch(real_image_batch, y_real), disc_loss_real)\n",
    "        disc_loss_refined = np.add(discriminator_model.train_on_batch(refined_image_batch, y_refined),\n",
    "                                   disc_loss_refined)\n",
    "\n",
    "    if not i % log_interval:\n",
    "        # plot batch of refined images w/ current refiner\n",
    "        figure_name = 'refined_image_batch_step_{}.png'.format(i)\n",
    "        print('Saving batch of refined images at adversarial step: {}.'.format(i))\n",
    "\n",
    "        synthetic_image_batch = get_image_batch(synthetic_generator)\n",
    "        plot_batch(\n",
    "            np.concatenate((synthetic_image_batch, refiner_model.predict_on_batch(synthetic_image_batch))),\n",
    "            os.path.join(cache_dir, figure_name),\n",
    "            label_batch=['Synthetic'] * batch_size + ['Refined'] * batch_size)\n",
    "\n",
    "        # log loss summary\n",
    "        print('Refiner model loss: {}.'.format(combined_loss / (log_interval * k_g * 2)))\n",
    "        print('Discriminator model loss real: {}.'.format(disc_loss_real / (log_interval * k_d * 2)))\n",
    "        print('Discriminator model loss refined: {}.'.format(disc_loss_refined / (log_interval * k_d * 2)))\n",
    "\n",
    "        combined_loss = np.zeros(shape=len(combined_model.metrics_names))\n",
    "        disc_loss_real = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "        disc_loss_refined = np.zeros(shape=len(discriminator_model.metrics_names))\n",
    "\n",
    "        # save model checkpoints\n",
    "        model_checkpoint_base_name = os.path.join(cache_dir, '{}_model_step_{}.h5')\n",
    "        refiner_model.save(model_checkpoint_base_name.format('refiner', i))\n",
    "        discriminator_model.save(model_checkpoint_base_name.format('discriminator', i))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SimGAN.ipynb",
   "version": "0.3.2",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
